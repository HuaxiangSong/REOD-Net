{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74692ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codes released by Huaxiang Song\n",
    "#email: cn11028719@huas.edu.cn\n",
    "#https://orcid.org/0000-0002-8235-0455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE ENSURE THAT THIS TECHNOLOGY IS NOT USED FOR MILITARY PURPOSES, \n",
    "#AND THE SOURCE CODE CAN BE REQUESTED FOR REASONABLE REASONS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aaa8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.optim import lr_scheduler \n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "\n",
    "import torchvision \n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.cuda.amp import autocast, GradScaler \n",
    "\n",
    "import time \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import detection.my_os_utilities as mytools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67611d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file_path = r'/home/jason/KerasPy/pretrained_weight'\n",
    "project_path=r'..'# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a87e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_dict = {\n",
    "    'efficientnet_b0':'efficientnet_b0_rwightman-3dd342df.pth',\n",
    "    \n",
    "    'efficientnet_b3':'efficientnet_b3_rwightman-cf984f9c.pth',  \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5855e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a93e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recordfiles():\n",
    "    global dataset_flag \n",
    "    dataset_flag = data_dir.split('/')[-2].split('_')[0]\n",
    "    \n",
    "    global TR_flag \n",
    "    TR_flag= 'TR'+ data_dir.split('/')[-1].split('-')[0] + data_dir[-1]\n",
    "    \n",
    "    global time_stamp \n",
    "    time_stamp = mytools.get_time_prefix('second')\n",
    "    \n",
    "    global dataset_log_str\n",
    "    dataset_log_str = f'{dataset_flag}_{TR_flag}_{time_stamp}_{model_selection}'\n",
    "\n",
    "    global training_logfile_name\n",
    "    training_logfile_name = f'Log_{dataset_log_str}.txt'#\n",
    "    \n",
    "    global model_sav_pth_filename\n",
    "    model_sav_pth_filename = f'{dataset_log_str}.pth'#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_workers --------------------------\n",
    "loader_workers = 2\n",
    "#----------------------\n",
    "pin_memory_bool=True\n",
    "use_amp = True\n",
    "#####optiizer_settings\n",
    "BNorm_decay_setting = 0.\n",
    "weight_decay_setting = 1e-08\n",
    "eps_setting = 1e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_setting_log():\n",
    "    global settings_log_1\n",
    "    settings_log_1 = '' \n",
    "    settings_log_1 += \\\n",
    "    f'sample_min_size ={sample_min_size}; sample_max_size ={sample_max_size};\\n'\n",
    "    settings_log_1 += \\\n",
    "    f'RHF_prob ={RHF_prob}; RVF_prob ={RVF_prob}; Rgray_prob ={Rgray_prob}; Rcontrast_prob ={Rcontrast_prob}; '\n",
    "    settings_log_1 += f'; Cjit_prob ={Cjit_prob}; Gausblur_prob = {Gausblur_prob}\\n'\n",
    "    settings_log_1 += f'\\nBNorm_decay_setting = {BNorm_decay_setting}; \\n;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_detection_model_b0(num_classes, pretrained=True):\n",
    "    global Roi_align_layers\n",
    "    \n",
    "    from torchvision.models.detection.rpn import AnchorGenerator\n",
    "    from torchvision.models.detection import FasterRCNN\n",
    "    from detection.backbone_utils import BackboneWithFPN\n",
    "\n",
    "    backbone = models.efficientnet_b0(weights=None)\n",
    "\n",
    "    if pretrained:\n",
    "        weights = torch.load(os.path.join(weights_file_path, model_weights_dict['efficientnet_b0']))\n",
    "    \n",
    "        print(f'Faster-RCNN-b0-fpn {backbone.load_state_dict(weights)}')\n",
    "\n",
    "    backbone = backbone.features\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    global return_layers,FPN_out_channels\n",
    "    return_layers = {'2': '0', '3': '1','5':'2','7':'3'}\n",
    "    \n",
    "    in_channels_list =  [ 24, 40, 112, 320]\n",
    "    \n",
    "    \n",
    "    backbone = BackboneWithFPN(backbone,return_layers, in_channels_list,FPN_out_channels)\n",
    "    \n",
    "    #####################\n",
    "   \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "            featmap_names = Roi_align_layers,\n",
    "            output_size=7,\n",
    "            sampling_ratio=2\n",
    "        )\n",
    "    ###########\n",
    "    global anchor_sizes\n",
    "    \n",
    "    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "    anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
    "    #######################################################\n",
    "\n",
    "    model = FasterRCNN(\n",
    "            backbone=backbone,\n",
    "            num_classes=num_classes,\n",
    "            rpn_anchor_generator=anchor_generator,\n",
    "            box_roi_pool=roi_pooler,\n",
    "            min_size = sample_min_size, max_size = sample_max_size\n",
    "        )  \n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98423d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_detection_model_b3(num_classes, pretrained=True):\n",
    "    global Roi_align_layers\n",
    "    \n",
    "    from torchvision.models.detection.rpn import AnchorGenerator\n",
    "    from torchvision.models.detection import FasterRCNN\n",
    "    from detection.backbone_utils import BackboneWithFPN\n",
    "\n",
    "    backbone = models.efficientnet_b3(weights=None)\n",
    "\n",
    "    if pretrained:\n",
    "        weights = torch.load(os.path.join(weights_file_path, model_weights_dict['efficientnet_b3']))\n",
    "        print(f'Faster-RCNN-b3-fpn {backbone.load_state_dict(weights)}')\n",
    "\n",
    "    backbone = backbone.features\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    global return_layers,FPN_out_channels\n",
    "    return_layers = {'2': '0', '3': '1','5':'2','7':'3'}\n",
    "    \n",
    "    in_channels_list =  [ 32, 48, 136, 384]\n",
    "    \n",
    "    \n",
    "    backbone = BackboneWithFPN(backbone,return_layers, in_channels_list,FPN_out_channels)\n",
    "    \n",
    "    #####################\n",
    "   \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "            featmap_names = Roi_align_layers,\n",
    "            output_size=7,\n",
    "            sampling_ratio=2\n",
    "        )\n",
    "    ###########\n",
    "    global anchor_sizes\n",
    "    \n",
    "    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "    anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
    "    #######################################################\n",
    "\n",
    "    model = FasterRCNN(\n",
    "            backbone=backbone,\n",
    "            num_classes=num_classes,\n",
    "            rpn_anchor_generator=anchor_generator,\n",
    "            box_roi_pool=roi_pooler,\n",
    "            min_size = sample_min_size, max_size = sample_max_size\n",
    "        )  \n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64a600-25d5-4520-84d5-725c576aa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    from torchvision.transforms import v2\n",
    "\n",
    "    ###################################################\n",
    "    global my_transforms_train, sample_min_size,data_dir\n",
    "    my_transforms_train = v2.Compose(\n",
    "        [\n",
    "            v2.ToImage(),\n",
    "            ###\n",
    "            v2.Resize(size=(sample_min_size,sample_min_size),antialias=True),\n",
    "            \n",
    "            v2.RandomApply(torch.nn.ModuleList([v2.ColorJitter(0.5,0.5,0.5),]),p = Cjit_prob),\n",
    "            ###\n",
    "            v2.RandomHorizontalFlip(p = RHF_prob), \n",
    "            ####\n",
    "            v2.RandomVerticalFlip(p = RVF_prob),\n",
    "            ####\n",
    "            v2.RandomGrayscale(p = Rgray_prob),\n",
    "            ###\n",
    "            v2.RandomAutocontrast(p = Rcontrast_prob),\n",
    "            ###\n",
    "            v2.RandomApply(torch.nn.ModuleList([v2.GaussianBlur(kernel_size = 5),]),p = Gausblur_prob),\n",
    "            ###\n",
    "            v2.ToDtype(torch.float32, scale = False),\n",
    "        ]\n",
    "    )\n",
    "    ######################################################3\n",
    "    global my_transforms_val\n",
    "    my_transforms_val = v2.Compose(\n",
    "        [            \n",
    "            v2.ToImage(), \n",
    "            v2.Resize(size=(sample_min_size,sample_min_size),antialias=True),\n",
    "            v2.ToDtype(torch.float32, scale = False),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa471e2-abc2-4ed7-9918-c99cfeaf2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    get_transforms()\n",
    "    ######################\n",
    "    global image_datasets\n",
    "    image_datasets = {}\n",
    "    train_anno_json = f'train-annotation-{data_dir[-1]}.json'\n",
    "    #################################################################################\n",
    "    image_datasets['train'] = datasets.CocoDetection(root= os.path.join(data_dir, 'train'), \n",
    "                                annFile = os.path.join(data_dir, 'anno_dir',train_anno_json),\n",
    "                                                     transforms = my_transforms_train,                                                 \n",
    "                                        )\n",
    "\n",
    "    #########################################################################################################\n",
    "    \n",
    "    if 'DIOR20' in data_dir or 'MAR20' in data_dir or 'ShipRS50' in data_dir:\n",
    "        data_dir_val = os.path.dirname(data_dir)\n",
    "\n",
    "        \n",
    "    image_datasets['val'] = datasets.CocoDetection(root = os.path.join(data_dir_val, 'val'), \n",
    "                                annFile = os.path.join(data_dir, 'anno_dir','val-annotation.json'), \\\n",
    "                                                       transforms = my_transforms_val, \n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691271e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_datasets():\n",
    "    global image_datasets\n",
    "    image_datasets['train'] = datasets.wrap_dataset_for_transforms_v2(image_datasets['train'], \\\n",
    "            target_keys= [\"boxes\", 'labels','image_id'])\n",
    "    \n",
    "    image_datasets['val'] = datasets.wrap_dataset_for_transforms_v2(image_datasets['val'], \\\n",
    "                target_keys= [\"boxes\", \"labels\",'area','image_id','iscrowd'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ccf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders():\n",
    "    wrap_datasets()\n",
    "    ##############\n",
    "    global dataloaders\n",
    "    dataloaders ={}\n",
    "    dataloaders['train']= torch.utils.data.DataLoader(image_datasets['train'], batch_size = train_Bsize,\n",
    "            shuffle=True, pin_memory=pin_memory_bool, persistent_workers = True,num_workers =loader_workers, \\\n",
    "                                                      collate_fn=lambda batch: tuple(zip(*batch)))\n",
    "    dataloaders['val'] =  torch.utils.data.DataLoader(image_datasets['val'], batch_size = val_Bsize,\\\n",
    "            shuffle=True, pin_memory=pin_memory_bool, persistent_workers = True,num_workers=loader_workers,\\\n",
    "                                                      collate_fn=lambda batch: tuple(zip(*batch))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_weight(module):\n",
    "    group_decay = []\n",
    "    group_no_decay = []\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "        elif isinstance(m, torch.nn.modules.conv._ConvNd):\n",
    "            group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "        elif isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "\n",
    "    assert len(list(module.parameters())) == len(group_decay) + len(group_no_decay)\n",
    "    groups = [dict(params=group_decay), dict(params=group_no_decay, weight_decay = BNorm_decay_setting)]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec441e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer():\n",
    "    \n",
    "    global model_ft,optimizer_ft,lr_init_Classify_model\n",
    "    \n",
    "    params_grouped = group_weight(model_ft)\n",
    "\n",
    "    optimizer_ft = optim.AdamW(params_grouped,\\\n",
    "                               lr=lr_init_Classify_model,betas=(0.9,0.999),eps = eps_setting,\\\n",
    "                               weight_decay = weight_decay_setting)\n",
    "    #############################################\n",
    "    global exp_lr_scheduler\n",
    "    exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer_ft, \n",
    "                                T_max=T_max_setting, eta_min=eta_min_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fabdec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model_ft():\n",
    "    global model_ft\n",
    "    global num_classes,sample_min_size,sample_max_size,loader_workers,train_Bsize,val_Bsize,image_input_size\n",
    "    if 'DIOR20' in data_dir:\n",
    "        num_classes = 21 \n",
    "        sample_min_size = image_input_size \n",
    "        sample_max_size = image_input_size\n",
    "\n",
    "    elif 'MAR20' in data_dir:\n",
    "        num_classes = 21 \n",
    "        sample_min_size = image_input_size \n",
    "        sample_max_size = image_input_size\n",
    "        \n",
    "    elif 'ShipRS50' in data_dir:\n",
    "        num_classes = 51 \n",
    "        sample_min_size = image_input_size \n",
    "        sample_max_size = image_input_size\n",
    "    #\n",
    "    if model_selection == 'Faster-RCNN-b3-fpn':\n",
    "        model_ft = my_get_detection_model_b3(num_classes)\n",
    "        loader_workers = 3\n",
    "    elif model_selection == 'Faster-RCNN-b0-fpn':\n",
    "        model_ft = my_get_detection_model_b0(num_classes)\n",
    "        loader_workers = 3   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c796de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_for_training():\n",
    "    global val_in_coco,logfile_path\n",
    "    \n",
    "    get_model_ft()\n",
    "    \n",
    "    get_datasets()\n",
    "\n",
    "    get_dataloaders()\n",
    "    \n",
    "    #get_mosaic_dataloader()\n",
    "    \n",
    "    val_in_coco = convert_to_coco_api(dataloaders['val'].dataset)\n",
    "    \n",
    "    get_optimizer()\n",
    "\n",
    "    get_recordfiles()\n",
    "\n",
    "    get_setting_log()\n",
    "\n",
    "    global logfile_path\n",
    "    \n",
    "    logfile_path = os.path.join(project_path,training_logfile_name)\n",
    "\n",
    "    print('model is ready for training!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_training_accum():\n",
    "    global val_in_coco,mosaic_prob,mosaic_switch\n",
    "    global accum_step\n",
    "    \n",
    "    my_scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
    "    #\n",
    "    with open(logfile_path,'a+') as record_file:\n",
    "        record_file.write(str(time.ctime())+'\\n')\n",
    "        record_file.write(f'T_model_selection = {model_selection} \\n study_target= {study_target} \\n')\n",
    "        record_file.write(f'training data_dir={data_dir} \\n')\n",
    "        record_file.write(f'train_Bsize = {train_Bsize*accum_step}; val_Bsize = {val_Bsize}; \\n')\n",
    "        record_file.write(f'train_Bsize_unaccum = {train_Bsize}; accum_step = {accum_step}; \\n')\n",
    "        record_file.write(f'Roi_align_layers = {Roi_align_layers}; anchor_sizes = {anchor_sizes}; \\n')\n",
    "        record_file.write(f'lr_init_Classify_model ={lr_init_Classify_model};  \\n')\n",
    "        record_file.write(f'lr_init_RCNN ={lr_init_RCNN};  \\n')\n",
    "        record_file.write(str(dataloaders['train'].dataset)+ '\\n')\n",
    "        record_file.write(str(dataloaders['val'].dataset)+ '\\n')\n",
    "        record_file.write(f'sample_min_size={sample_min_size}, sample_max_size={sample_min_size};\\n')\n",
    "        record_file.write( f'return_layers ={return_layers}; \\n{para_settings}')\n",
    "        record_file.write(f'T_max={T_max_setting} \\n')\n",
    "        record_file.write(f'mosaic_prob={mosaic_prob} \\n')\n",
    "        record_file.write(f'eta_min={eta_min_setting} \\n\\n')\n",
    "        record_file.write('-' * 10+'DA Hyper-parameter setting'+'-' * 10+'\\n')\n",
    "        record_file.write(f'{settings_log_1} \\n') \n",
    "\n",
    "        record_file.write('-' * 10+'start training'+'-' * 10+'\\n')\n",
    "\n",
    "    ###########################################\n",
    "    since = time.time()\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(train_epochs):\n",
    "        epoch_since = time.time()\n",
    "\n",
    "        mosaic_switch = True\n",
    "            \n",
    "        # training for one epoch\n",
    "        log,mosaic_counter = train_one_epoch_accum(model_ft, optimizer_ft, dataloaders['train'], device, epoch, \\\n",
    "                              print_freq=100, scaler= my_scaler,mosaic_prob = mosaic_prob)\n",
    "\n",
    "        # update the learning rate\n",
    "        if ((epoch + 1)%accum_step) == 0:\n",
    "            exp_lr_scheduler.step()\n",
    "\n",
    "        with open(logfile_path,'a+') as record_file:\n",
    "            training_time_elapsed = time.time() - epoch_since\n",
    "            training_time_elapsed_str = \\\n",
    "             f'training complete in {training_time_elapsed//60:.0f}m {training_time_elapsed%60:.0f}s\\n'\n",
    "            record_file.write(f'epoch {epoch+1} of {train_epochs}:\\n')\n",
    "            record_file.write(f'{str(log)}\\n{training_time_elapsed_str}')\n",
    "            record_file.write('-' * 10+'\\n')\n",
    "            \n",
    "        print(f'\\nmosaic_times:{mosaic_counter}\\n')\n",
    "        torch.cuda.empty_cache()\n",
    "    #----------------------------------------\n",
    "        if epoch < epoch_skip_testing:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # evaluate on the test dataset\n",
    "        result = evaluate_resize(model_ft, dataloaders['val'], device=device)\n",
    "\n",
    "        if result.coco_eval['bbox'].stats[1] > best_acc:\n",
    "            best_acc = result.coco_eval['bbox'].stats[1]\n",
    "            best_epoch = epoch\n",
    "            #####\n",
    "            torch.save(model_ft.state_dict(),os.path.join(project_path,model_sav_pth_filename))\n",
    "\n",
    "        print('-' * 30+'\\n')\n",
    "        best_rec = f'best acc: {best_acc} in epoch {best_epoch +1 }\\n'\n",
    "        print(best_rec)\n",
    "        \n",
    "        epoch_time_elapsed = time.time() - epoch_since\n",
    "        epoch_time_elapsed_str = \\\n",
    "             f'epoch complete in {epoch_time_elapsed//60:.0f}m {epoch_time_elapsed%60:.0f}s\\n'\n",
    "\n",
    "        with open(logfile_path,'a+') as record_file:  \n",
    "            record_file.write('-' * 30+'\\n')\n",
    "            rec = result.coco_eval['bbox'].summarize_record\n",
    "            record_file.write(f'{rec}\\n')\n",
    "            record_file.write(best_rec)\n",
    "            record_file.write(epoch_time_elapsed_str)\n",
    "            record_file.write('-' * 30+'\\n')\n",
    "    #end for---------------------------------------------------\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    with open(logfile_path,'a+') as record_file:\n",
    "        record_file.write('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)+'\\n')\n",
    "        record_file.write(f'best acc: {best_acc} in epoch {best_epoch +1 }\\n')\n",
    "    #### \n",
    "    print(('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)+'\\n'))\n",
    "    print((f'best acc: {best_acc} in epoch {best_epoch +1 }\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_accum(model, optimizer, data_loader, device, epoch, print_freq, scaler=None,mosaic_prob=0.5):\n",
    "    import math,gc,random\n",
    "    from detection.utils import MetricLogger,SmoothedValue,reduce_dict\n",
    "    from detection.coco_utils import get_coco_api_from_dataset\n",
    "    \n",
    "    global accum_step,mosaic_switch,using_two_mosaic,mosaic_combination \n",
    "    model.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "    mosaic_counter = 0\n",
    "    \n",
    "    batch_idx = 1 \n",
    "    total_batches_num = len(data_loader)\n",
    "    \n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        images = list(image for image in images)\n",
    "        targets = [{k: v if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        #print(f'target length: {len(targets)}')\n",
    "        if np.random.rand(1) < mosaic_prob and mosaic_switch:\n",
    "            batchsize = len(images)\n",
    "            resolution = images[0].shape[-1]\n",
    "            \n",
    "            mosaic_choice = random.sample(mosaic_combination,1)\n",
    "            if mosaic_choice == [4]:\n",
    "                if get_four_mosaic_image_and_boxes(images,targets,resolution=resolution,batchsize=batchsize) \\\n",
    "                  == True:\n",
    "                    mosaic_counter+=1\n",
    "            elif mosaic_choice == [9]:\n",
    "                if get_nine_mosaic_image_and_boxes(images,targets,resolution=resolution,batchsize=batchsize) \\\n",
    "                  == True:\n",
    "                    mosaic_counter+=1\n",
    "            elif mosaic_choice == [3]:\n",
    "                if get_three_mosaic_image_and_boxes(images,targets,resolution=resolution,batchsize=batchsize) \\\n",
    "                  == True:\n",
    "                    mosaic_counter+=1\n",
    "            elif mosaic_choice == [0]:\n",
    "                if get_classic_mosaic_image_and_boxes(images,targets,resolution=resolution,batchsize=batchsize) \\\n",
    "                  == True:\n",
    "                    mosaic_counter+=1\n",
    "        \n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled = scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            losses = losses/(accum_step*1.0)\n",
    "            \n",
    "        del images\n",
    "        \n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        #optimizer.zero_grad()\n",
    "        #if scaler is not None:\n",
    "        scaler.scale(losses).backward()\n",
    "        if ((batch_idx % accum_step) == 0) or (batch_idx ==total_batches_num):  \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batch_idx += 1\n",
    "   \n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "        del targets,loss_dict, losses\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return metric_logger,mosaic_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate_resize(model, data_loader, device):\n",
    "    from detection.engine import _get_iou_types\n",
    "    from detection.coco_eval import CocoEvaluator\n",
    "    from detection.utils import MetricLogger\n",
    "    \n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = \"Test:\"\n",
    "\n",
    "    #coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    global val_in_coco\n",
    "    coco = val_in_coco\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        \n",
    "        '''for target in targets:\n",
    "            for i in range(len(target['boxes'])):\n",
    "                x1,y1,x2,y2 = target['boxes'][i]\n",
    "                target['area'][i] = abs((x2-x1)*(y2-y1))'''  \n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ee7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_cycle_accum():\n",
    "    \n",
    "    global data_dir\n",
    "    \n",
    "    for i in range(len(data_dir_list)):        \n",
    "        \n",
    "        data_dir = data_dir_list[i]\n",
    "\n",
    "        init_for_training()\n",
    "\n",
    "        my_training_accum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c269a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_QDA_hyperparameters():\n",
    "    global Cjit_prob,RHF_prob,RVF_prob,Rgray_prob,Rcontrast_prob,Gausblur_prob\n",
    "    Cjit_prob = 0.\n",
    "    RHF_prob = 0.\n",
    "    RVF_prob = 0.\n",
    "    Rgray_prob = 0.\n",
    "    Rcontrast_prob = 0.\n",
    "    Gausblur_prob = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_coco_api(ds):\n",
    "    from pycocotools.coco import COCO\n",
    "    \n",
    "    coco_ds = COCO()\n",
    "    # annotation IDs need to start at 1, not 0, see torchvision issue #1530\n",
    "    ann_id = 1\n",
    "    dataset = {\"images\": [], \"categories\": [], \"annotations\": []}\n",
    "    categories = set()\n",
    "    for img_idx in range(len(ds)):\n",
    "        # find better way to get target\n",
    "        # targets = ds.get_annotations(img_idx)\n",
    "        img, targets = ds[img_idx]       \n",
    "                \n",
    "        image_id = targets[\"image_id\"]\n",
    "        img_dict = {}\n",
    "        img_dict[\"id\"] = image_id\n",
    "        img_dict[\"height\"] = img.shape[-2]\n",
    "        img_dict[\"width\"] = img.shape[-1]\n",
    "        dataset[\"images\"].append(img_dict)\n",
    "        bboxes = targets[\"boxes\"].clone()\n",
    "        bboxes[:, 2:] -= bboxes[:, :2]\n",
    "        bboxes = bboxes.tolist()\n",
    "        labels = targets[\"labels\"].tolist()\n",
    "        areas = targets[\"area\"]#.tolist()\n",
    "        if 'iscrowd' in targets:\n",
    "            iscrowd = targets[\"iscrowd\"]\n",
    "        if \"masks\" in targets:\n",
    "            masks = targets[\"masks\"]\n",
    "            # make masks Fortran contiguous for coco_mask\n",
    "            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)\n",
    "        if \"keypoints\" in targets:\n",
    "            keypoints = targets[\"keypoints\"]\n",
    "            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()\n",
    "        num_objs = len(bboxes)\n",
    "        for i in range(num_objs):\n",
    "            ann = {}\n",
    "            ann[\"image_id\"] = image_id\n",
    "            ann[\"bbox\"] = bboxes[i]\n",
    "            ann[\"category_id\"] = labels[i]\n",
    "            categories.add(labels[i])\n",
    "            #ann[\"area\"] = areas[i]\n",
    "            ###fix height & width\n",
    "            box_height = bboxes[i][2]\n",
    "            box_width = bboxes[i][3]\n",
    "            ann[\"area\"] = box_height * box_width\n",
    "            ######            \n",
    "            ann[\"iscrowd\"] = iscrowd[i]\n",
    "            ann[\"id\"] = ann_id\n",
    "            if \"masks\" in targets:\n",
    "                ann[\"segmentation\"] = coco_mask.encode(masks[i].numpy())\n",
    "            if \"keypoints\" in targets:\n",
    "                ann[\"keypoints\"] = keypoints[i]\n",
    "                ann[\"num_keypoints\"] = sum(k != 0 for k in keypoints[i][2::3])\n",
    "            dataset[\"annotations\"].append(ann)\n",
    "            ann_id += 1\n",
    "    dataset[\"categories\"] = [{\"id\": i} for i in sorted(categories)]\n",
    "    coco_ds.dataset = dataset\n",
    "    coco_ds.createIndex()\n",
    "    return coco_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classic_mosaic_image_and_boxes(images,targets, resolution,batchsize=8,\\\n",
    "                                    minfrac=0.25, maxfrac=0.75,size_limit=2):\n",
    "    import random\n",
    "    from torchvision.tv_tensors._bounding_boxes import BoundingBoxes\n",
    "    \n",
    "    if batchsize<4: return batchsize\n",
    "    \n",
    "    s = resolution\n",
    "\n",
    "    \n",
    "    image_batch = []\n",
    "    box_batch = []\n",
    "    label_batch = []\n",
    "    \n",
    "    for ii in range(batchsize):\n",
    "        #mosaic_size = s\n",
    "        xc, yc = np.random.randint(resolution * minfrac, resolution * maxfrac, (2,)) \n",
    "        current_index = ii\n",
    "        # random other 3 sample (could be the same too...)\n",
    "        indice_list = [i for i in range(batchsize)]\n",
    "        _ = indice_list.remove(current_index)\n",
    "        indices = [current_index] + random.sample(indice_list, 3)\n",
    "\n",
    "        mosaic_image = torch.zeros(3,s, s)\n",
    "        final_boxes  = None\n",
    "        final_labels = None\n",
    "    \n",
    "        for i, index in enumerate(indices):\n",
    "            \n",
    "            image = images[index].clone()\n",
    "            \n",
    "            boxes = targets[index]['boxes'].clone()\n",
    "            \n",
    "            labels = targets[index]['labels'].clone()\n",
    "    \n",
    "\n",
    "            if i == 0:    # top left\n",
    "                x1o, y1o, x2o, y2o =  0,  0, xc, yc #coordinates in original image\n",
    "                x1m, y1m, x2m, y2m =  0,  0, xc, yc #coordinates in mosaic image \n",
    "            elif i == 1:  # top right\n",
    "                x1o, y1o, x2o, y2o =  xc,  0, s, yc #coordinates in original image\n",
    "                x1m, y1m, x2m, y2m =  xc,  0, s, yc #coordinates in mosaic image \n",
    "            elif i == 2:  # bottom left\n",
    "                x1o, y1o, x2o, y2o =  0,  yc, xc, s #coordinates in original image\n",
    "                x1m, y1m, x2m, y2m =  0,  yc, xc, s #coordinates in mosaic image \n",
    "            elif i == 3:  # bottom right\n",
    "                x1o, y1o, x2o, y2o =  xc,  yc, s, s #coordinates in original image\n",
    "                x1m, y1m, x2m, y2m =  xc,  yc, s, s #coordinates in mosaic image \n",
    "\n",
    "            # cut image, save boxes\n",
    "         \n",
    "            mosaic_image[:,y1m:y2m, x1m:x2m] = image[:,y1o:y2o, x1o:x2o]\n",
    "            if i == 0:\n",
    "                #boxes\n",
    "                final_boxes=boxes\n",
    "                final_boxes[:, 0:3:2] = torch.clamp(final_boxes[:, 0:3:2], x1m, x2m)\n",
    "                final_boxes[:, 1:4:2] = torch.clamp(final_boxes[:, 1:4:2], y1m, y2m)\n",
    "                w = (final_boxes[:,2] - final_boxes[:,0])\n",
    "                h = (final_boxes[:,3] - final_boxes[:,1])\n",
    "                final_boxes = final_boxes[(w >= size_limit) & (h >= size_limit)]\n",
    "                #label\n",
    "                final_labels=labels\n",
    "                final_labels = final_labels[(w >= size_limit) & (h >= size_limit)]\n",
    "            elif i == 1:\n",
    "                current_boxes=boxes\n",
    "                current_boxes[:, 0:3:2] = torch.clamp(current_boxes[:, 0:3:2], x1m, x2m)\n",
    "                current_boxes[:, 1:4:2] = torch.clamp(current_boxes[:, 1:4:2], y1m, y2m)\n",
    "                w = (current_boxes[:,2] - current_boxes[:,0])\n",
    "                h = (current_boxes[:,3] - current_boxes[:,1])\n",
    "                current_boxes = current_boxes[(w >= size_limit) & (h >= size_limit)]\n",
    "                #label\n",
    "                current_labels=labels\n",
    "                current_labels = current_labels[(w >= size_limit) & (h >= size_limit)]\n",
    "                #concat\n",
    "                final_boxes=torch.cat((final_boxes,current_boxes),0)\n",
    "                final_labels=torch.cat((final_labels,current_labels),0)\n",
    "            elif i == 2:\n",
    "                current_boxes=boxes\n",
    "                current_boxes[:, 0:3:2] = torch.clamp(current_boxes[:, 0:3:2], x1m, x2m)\n",
    "                current_boxes[:, 1:4:2] = torch.clamp(current_boxes[:, 1:4:2], y1m, y2m)\n",
    "                w = (current_boxes[:,2] - current_boxes[:,0])\n",
    "                h = (current_boxes[:,3] - current_boxes[:,1])\n",
    "                current_boxes = current_boxes[(w >= size_limit) & (h >= size_limit)]\n",
    "                #label\n",
    "                current_labels=labels\n",
    "                current_labels = current_labels[(w >= size_limit) & (h >= size_limit)]\n",
    "                #concat\n",
    "                final_boxes=torch.cat((final_boxes,current_boxes),0)\n",
    "                final_labels=torch.cat((final_labels,current_labels),0)\n",
    "            elif i == 3:\n",
    "                current_boxes=boxes\n",
    "                current_boxes[:, 0:3:2] = torch.clamp(current_boxes[:, 0:3:2], x1m, x2m)\n",
    "                current_boxes[:, 1:4:2] = torch.clamp(current_boxes[:, 1:4:2], y1m, y2m)\n",
    "                w = (current_boxes[:,2] - current_boxes[:,0])\n",
    "                h = (current_boxes[:,3] - current_boxes[:,1])\n",
    "                current_boxes = current_boxes[(w >= size_limit) & (h >= size_limit)]\n",
    "                #label\n",
    "                current_labels=labels\n",
    "                current_labels = current_labels[(w >= size_limit) & (h >= size_limit)]\n",
    "                #concat\n",
    "                final_boxes=torch.cat((final_boxes,current_boxes),0)\n",
    "                final_labels=torch.cat((final_labels,current_labels),0)\n",
    "                \n",
    "        \n",
    "        #break\n",
    "        image_batch.append(mosaic_image)\n",
    "        box_batch.append(final_boxes)\n",
    "        label_batch.append(final_labels)\n",
    "        \n",
    "    #end loop\n",
    "    images[:][:] = image_batch[:][:]\n",
    "    for i in range(batchsize):\n",
    "        targets[i]['boxes']=box_batch[i].to(device)\n",
    "        targets[i]['labels']=label_batch[i].to(device)  \n",
    "        \n",
    "    del image_batch \n",
    "    del box_batch \n",
    "    del label_batch \n",
    "    \n",
    "    global classic_mosaic_tag\n",
    "    if classic_mosaic_tag == False:\n",
    "        print('classic mosaic running... ')\n",
    "        classic_mosaic_tag = True\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mosaic_setting():\n",
    "    global mosaic_combination,using_three_mosaic,using_four_mosaic, using_nine_mosaic,using_classic_mosaic\n",
    "    mosaic_combination = []\n",
    "    if using_three_mosaic: \n",
    "        mosaic_combination.append(3) \n",
    "    if using_four_mosaic: \n",
    "        mosaic_combination.append(4)\n",
    "    if using_nine_mosaic: \n",
    "        mosaic_combination.append(9) \n",
    "    if using_classic_mosaic:\n",
    "        mosaic_combination.append(0) \n",
    "\n",
    "    global three_mosaic_tag,four_mosaic_tag, nine_mosaic_tag,classic_mosaic_tag\n",
    "    three_mosaic_tag = False\n",
    "    four_mosaic_tag = False\n",
    "    nine_mosaic_tag = False\n",
    "    classic_mosaic_tag = False\n",
    "\n",
    "    if len(mosaic_combination) == 0: \n",
    "        raise ValueError('mosaic_combination is empty')\n",
    "    \n",
    "    global para_settings\n",
    "    para_settings = f'FPN_out_channels = {FPN_out_channels};\\nusing_four_mosaic = {using_four_mosaic};\\n'\n",
    "    para_settings+= f'using_three_mosaic = {using_three_mosaic}\\nusing_nine_mosaic = {using_nine_mosaic}\\n'\n",
    "    para_settings+= f'using_classic_mosaic = {using_classic_mosaic}\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a041d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Bsize = 8 #Ensuring the codes work fine on your computer when your GPU memory is less than 16G\n",
    "accum_step = 4\n",
    "\n",
    "epoch_skip_testing = 0\n",
    "\n",
    "train_epochs = 72 * 4\n",
    "\n",
    "T_max_setting = 72    \n",
    "\n",
    "lr_init_Classify_model = 1e-4\n",
    "\n",
    "lr_init_RCNN = 1e-4\n",
    "\n",
    "eta_min_setting = lr_init_Classify_model * 0.01\n",
    "\n",
    "FPN_out_channels = 256\n",
    "\n",
    "val_Bsize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_list = [r'/home/jason/data/MAR20_dataset/30%-Train-ratio-A',\n",
    "                 \n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection = 'Faster-RCNN-b3-fpn'\n",
    "\n",
    "study_target = 'GSConv-in-FPN_349_mosaic_LR=1E-4'\n",
    "\n",
    "Roi_align_layers = ['0','1','2','3',]\n",
    "\n",
    "anchor_sizes = ((24,), (48,), (96,), (144,), (192,))\n",
    "\n",
    "using_resize = True\n",
    "\n",
    "image_input_size = 640\n",
    "\n",
    "mosaic_prob = 0.3\n",
    "\n",
    "init_QDA_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_skip_testing = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97862024",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_classic_mosaic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be9780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE ENSURE THAT THIS TECHNOLOGY IS NOT USED FOR MILITARY PURPOSES, \n",
    "#AND THE SOURCE CODE CAN BE REQUESTED FOR REASONABLE REASONS.\n",
    "using_four_mosaic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54800172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE ENSURE THAT THIS TECHNOLOGY IS NOT USED FOR MILITARY PURPOSES, \n",
    "#AND THE SOURCE CODE CAN BE REQUESTED FOR REASONABLE REASONS.\n",
    "using_three_mosaic = False\n",
    "\n",
    "using_nine_mosaic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7afb226",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mosaic_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993fab5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_on_cycle_accum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00685af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e86f5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.system('shutdown +1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
